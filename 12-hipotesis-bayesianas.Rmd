
<!-- # Pruebas de hipótesis bayesianas -->

<!-- ## Pruebas de hipótesis bayesianas -->

<!-- Suponga que $\Omega = \{\theta_0,\theta_1\}$. Si $\theta = \theta_i$ -->
<!-- $(i = 0,1)$, $X_1,\dots, X_n\sim f_i(x)$. Considere las hipótesis -->

<!-- \[H_0: \theta = \theta_0 \text{ vs } H_1: \theta =\theta_1.\] -->

<!-- Hay dos decisiones, $d_0:$ no rechazo $H_0$ y $d_1:$ rechazo $H_0$. -->

<!-- Asuma que si selecciono $d_1$ cuando $H_0$ es cierto, la pérdida es $w_0$ -->
<!-- unidades. Por -->
<!-- el contrario, si selecciono $d_1$ cuando $H_0$ es cierto, la pérdida es $w_1$ -->
<!-- unidades.  -->

<!-- Como un ejemplo, se podría pensar  \(w_0\) y \(w_1\)  como la pérdida en colones -->
<!-- que ocurre cuando me equivoco al tomar una desición errónea. -->



<!-- |          | $d_0$ | $d_1$ | -->
<!-- |----------|-------|-------| -->
<!-- |$\theta_0$| 0     | $w_0$ | -->
<!-- |$\theta_1$| $w_1$ | 0     | -->


<!-- **Ejemplo:** Recuerden el ejemplo del administrador del \emph{call center} que -->
<!-- tenía que decidir si \(f_0\) o \(f_1\) era la distribución correcta para la -->
<!-- entrada de llamadas. El administrador observa lo siguiente: \(f_1\) asigna más -->
<!-- probabilidad a los tiempos de llamadas largos y extremadamente pequeños con -->
<!-- respecto a \(f_0\).  -->

<!-- Suponga que el costo de modelar tiempos de servicio extremadamente grandes como -->
<!-- menos probables de lo que realmente son es el mismo que el costo de modelar -->
<!-- tiempos de servicio extremadamente grandes para que sean más probables de lo que -->
<!-- realmente son.  -->


<!-- Entonces el administrador concluye que no hay diferencia en el costo si el se -->
<!-- escoge la distribución incorrecta para modelar el tiempo, de modo que la pérdida -->
<!-- para la empresa es \(w_0=w_1\) colones independientemente de la escogencia. -->
<!-- \(\hfill\blacktriangleleft\) -->



<!-- Defina las distribuciones previas $\pi_0 = \mathbb P[H_0 \text{ es cierta}] -->
<!-- 	= \mathbb P[H_0]$ y $\pi_1 = \mathbb P[H_1 \text{ es cierta}] = 1 -\mathbb -->
<!-- 	P[H_0]$. -->

<!-- **Ejemplo:** Para el caso del _call center_ suponga que el administrador asigna -->
<!-- igual probabilidad de equivocarse, por lo tanto define \(\pi_0=\pi_1 = -->
<!-- 1/2\). -->



<!-- Recuerden que \(\delta\) es la regla de decisión de la prueba. El valor esperado -->
<!-- de la pérdida corresponde a -->

<!-- \[ r(\delta) = \mathbb E[\text{Pérdida}|H_0] \mathbb P[H_0] + \mathbb -->
<!-- 	E[\text{Pérdida}|H_1]\mathbb P[H_1].\] -->

<!-- Asumiendo que $H_0$ es cierto, -->

<!-- \begin{align*} -->
<!-- 	\mathbb E[\text{Pérdida}|H_0] & = w_0\cdot \mathbb -->
<!-- 	P[\text{Seleccione } d_1|\theta_0] + 0\cdot \mathbb P[\text{Seleccione } -->
<!-- 	d_0|\theta_0]                                      \\  -->
<!--     & = w_0 \cdot \text{Error Tipo I} = w_0\ \alpha(\delta). -->
<!-- \end{align*} -->
<!-- Por el otro lado, -->

<!-- \begin{align*} -->
<!-- 	\mathbb E[\text{Pérdida}|H_1] & = 0\cdot \mathbb P[\text{Seleccione } d_1|\theta_0] + w_1\cdot \mathbb P[\text{Seleccione } d_0|\theta_0] \\ -->
<!-- & = w_1 \cdot \text{Error Tipo II} = w_1\ \beta(\delta). -->
<!-- \end{align*} -->

<!-- Entonces $r(\delta) = w_0\alpha(\delta) + w_1\beta(\delta)$. -->

<!-- El objetivo siempre será minimizar \(r(\delta)\) ya que sería la pérdida -->
<!-- total. A esta minimización es el procedimiento de prueba bayesiana.  -->

<!-- Por el teorema de Neyman-Pearson y tomando $a=\pi_0w_0$ y $b=\pi_1w_1$, la regla -->
<!-- de decisión $\delta$ que soluciona el problema es: -->

<!-- \begin{align*} -->
<!-- \text{Rechazo }H_0 \text{ si }  -->
<!-- & \pi_0w_0f_0(x) \leq \pi_1w_1f_1(x). \\ -->
<!-- & \dfrac{f_1(x)}{f_0(x)} \geq \dfrac{\pi_0w_0}{\pi_1w_1} \\ -->
<!-- & \dfrac{f_1(x)}{f_0(x)} \geq \dfrac{a}{b} \\ -->
<!-- & \dfrac{f_1(x)}{f_0(x)} \geq k \\ -->
<!-- \end{align*} -->

<!-- **Nota**. La decisión $\delta$ es invariante a multiplicaciones por escalar en el -->
<!-- costo. -->



<!-- <!-- Recordemos como estimar la distribuciones posteriores en el caso bayesiano --> -->

<!-- <!-- \[\pi(\theta_0|x) = \dfrac{\pi_0f_0(x)}{\pi_0f_0(x)+\pi_1f_1(x)}\] --> -->
<!-- <!-- \[\pi(\theta_1|x) = \dfrac{\pi_1f_1(x)}{\pi_0f_0(x)+\pi_1f_1(x)}\] --> -->

<!-- <!-- En este casos la esperanza de la pérdida se calcula como:  --> -->

<!-- <!-- \[\mathbb E[\text{Pérdida}|x] = \mathbb E[\text{Pérdida}|\theta_0,x] + \mathbb E[\text{Pérdida}|\theta_1,x].\] --> -->

<!-- <!-- Si escogemos  esta decisión obtenemos que  $\delta = d_0$, --> -->

<!-- <!-- \[\mathbb E_\delta[\text{Pérdida}|X] = w_1\pi(\theta_1|x) = \dfrac{w_1\pi_1f_1(x)}{\pi_0f_0(x)+\pi_1f_1(x)}. \] --> -->

<!-- <!-- En el otro caso obtenemos que  $\delta = d_1$, --> -->

<!-- <!-- \[\mathbb E_\delta[\text{Pérdida}|X] = \dfrac{w_0\pi_0f_0(x)}{\pi_0f_0(x)+\pi_1f_1(x)}. \] --> -->

<!-- <!-- Minimizar $\mathbb E[\text{Pérdida}|x]$ con respecto a $\delta$ es equivalente a --> -->
<!-- <!-- rechazar $H_0$ bajo el criterio anterior. --> -->

<!-- <!-- **Conclusión**: es equivalente construir la decisión en cualquiera de los dos --> -->
<!-- <!-- criterios (previa o probabilidad posterior). --> -->

<!-- <!-- c. Rechazo $H_0$ si --> -->
<!-- <!-- $\mathbb P[H_0 \text{ es cierto}|X] \leq \dfrac{w_1}{w_0+w_1}.$ --> -->

<!-- <!-- Rechazamos $H_0$ si --> -->
<!-- <!-- \[\dfrac{w_0\pi_0f_0(x)}{\pi_0f_0(x)+\pi_1f_1(x)}\leq \dfrac{w_1\pi_1f_1(x)}{\pi_0f_0(x)+\pi_1f_1(x)}.\] --> -->

<!-- <!-- Rechazamos $H_0$ si  --> -->
<!-- <!-- \begin{align*} --> -->
<!-- <!-- \dfrac{w_0\pi_0f_0(x)}{\pi_0f_0(x)+\pi_1f_1(x)}  --> -->
<!-- <!-- & \leq \dfrac{w_1\pi_1f_1(x)}{\pi_0f_0(x)+\pi_1f_1(x)} \\ --> -->
<!-- <!-- w_0\pi_0f_0(x)   --> -->
<!-- <!-- & \leq w_1\pi_1f_1(x) \\ --> -->
<!-- <!-- \dfrac{w_0\pi_0}{w_1\pi_1} &\leq \dfrac{f_1(x)}{f_0(x)} --> -->
<!-- <!-- \end{align*} --> -->

<!-- <!-- Entonces --> -->

<!-- <!-- \[w_0\mathbb P[H_0|x] \leq w_1\mathbb P[H_1|x] = w_1[1-\mathbb P[H_0|x]] --> -->
<!-- <!-- \implies\mathbb P[H_0|x]\leq \dfrac{w_1}{w_0+w_1}. \] --> -->



<!-- **Ejemplo:** En el ejemplo del _call center_ con las condiciones planteadas la -->
<!-- regla de decisión bayesiana sería  -->

<!-- \begin{align*} -->
<!-- \dfrac{w_0\pi_0}{w_1\pi_1} &\leq \dfrac{f_1(x)}{f_0(x)} \\ -->
<!-- 1 &\leq \dfrac{f_1(x)}{f_0(x)} \\ -->
<!-- \end{align*} -->


<!-- ```{r } -->

<!-- ``` -->




<!-- ## Hipótesis de una cola -->

<!-- *Caso general*: $H_0: \theta \in \Omega_0$ vs $H_1: \theta \in \Omega_1$. -->
<!-- Asuma la misma función de pérdida $L(\theta,d_i)$: -->

<!-- |     | $d_0$ | $d_1$ | -->
<!-- |-----|-------|-------| -->
<!-- |$H_0$| 0     | $w_0$ | -->
<!-- |$H_1$| $&w_1$ | 0     | -->

<!-- y considere la hipótesis $H_0: \theta \leq \theta_0$ vs $H_1: \theta > \theta_0$. -->

<!-- **Teorema**. Suponga que $f_n(x|\theta)$ tiene un cociente de verosimilitud -->
<!-- monótono con respecto al estadístico $T=r(x)$. Es decir, $R(x) = -->
<!-- 	\dfrac{f_n(x|\theta_1)}{f_n(x|\theta_0)} = g(r(x))$ con $g(r(x))$ monótono con -->
<!-- respecto a $r(x)$. -->

<!-- Asuma la función de pérdida anterior. Entonces el procedimiento bayesiano de -->
<!-- prueba rechaza $H_0$ cuando $T\geq c$. -->

<!-- **Definición**. Si $f_n(x|\theta)$ es una verosimilitud y $T=r(x)$ un -->
<!-- estadístico, decimos que $f_n(x|\theta)$ tiene un MLR con respecto a $T$ si para -->
<!-- $\theta_1,\theta_2 \in \Omega$ tales que $\theta_1 <\theta_2$, -->
<!-- $\dfrac{f_n(x|\theta_2)}{f_n(x|\theta_1)}$ depende de $x$ a través de $r(x)$ y -->
<!-- es una función monótona de $r(x)$. -->

<!-- **Ejemplo**. $X_1,\dots, X_n \sim N(\mu,\sigma^2)$, $\sigma^2$ conocido. Si -->
<!-- $\mu_1<\mu_2$: -->

<!-- \begin{align*} -->
<!-- 	\dfrac{f_n(x|\mu_2)}{f_n(x|\mu_1)} & = -->
<!-- 	\dfrac{(2\pi\sigma^2)^{-n/2}\exp\bigg[-\dfrac 1{2\sigma^2}\sum -->
<!-- 			(X_i-\mu_2)^2\bigg]}{(2\pi\sigma^2)^{-n/2}\exp\bigg[-\dfrac 1{2\sigma^2}\sum -->
<!-- 	(X_i-\mu_1)^2\bigg]}                   \\ & = \exp\bigg[\dfrac{n(\mu_2-\mu_1)}{\sigma^2}\bar X_n -->
<!-- 		-\dfrac 12 (\mu_2+\mu_1))\bigg] = g(T) -->
<!-- \end{align*} -->

<!-- Entonces $g(T)$ es monótono creciente con respecto a $T=\bar X_n$ y por lo tanto -->
<!-- tiene un MLE con respecto a $\bar X_n$. -->

<!-- *Prueba*. Recuerde que -->

<!-- \[\pi(\theta|x) = \dfrac{f_n(x|\theta)\pi(\theta)}{\int f_n(x|\psi)\pi(\psi)d\psi}\] -->

<!-- \begin{align*} -->
<!-- 	\mathcal L(x) = \dfrac{\mathbb E[\text{Pérdida}|x,d_0]}{\mathbb E[\text{Pérdida}|x,d_1]} & = \dfrac{\dfrac 1{\text{cte}} \displaystyle\int_{\theta_0}^\infty w_1f_n(x|\theta)\pi(\theta)d\theta}{\dfrac 1{\text{cte}} \displaystyle\int_{-\infty}^{\theta_0} w_0f_n(x|\theta)\pi(\theta)d\theta} \\ & = \dfrac{w_1}{w_0}\dfrac{ \displaystyle\int_{\theta_0}^\infty f_n(x|\theta)\pi(\theta)d\theta}{ \displaystyle\int_{-\infty}^{\theta_0} f_n(x|\theta)\pi(\theta)d\theta}  \geq 1 -->
<!-- \end{align*} -->

<!-- Buscamos rechazar si $l(x)\geq 1$ (prueba bayesiana) y si existe una función -->
<!-- monótona tal que $l(x)\geq 1 \Leftrightarrow T\geq c$, entonces ambas pruebas -->
<!-- son iguales. Basta con probar que $l(x)$ es una función monótona creciente de -->
<!-- $T$. -->

<!-- Sea $X_1,X_2\in \mathcal X$ tal que $r(X_1)\leq r(X_2)$. Entonces -->

<!-- \[ l(X_1)-l(X_2) = \dfrac{w_1 \displaystyle\int_{\theta_0}^\infty f_n(x_1|\theta)\pi(\theta)d\theta}{w_0 \displaystyle\int_{-\infty}^{\theta_0} f_n(x_1|\theta)\pi(\theta)d\theta} -\dfrac{w_1 \displaystyle\int_{\theta_0}^\infty f_n(x_2|\theta)\pi(\theta)d\theta}{w_0 \displaystyle\int_{-\infty}^{\theta_0} f_n(x_2|\theta)\pi(\theta)d\theta}\leq 0. \] -->

<!-- Si simplificamos la expresión en una sola fracción, el numerador es de la forma -->

<!-- \[\int_{\theta_0}^\infty\int_{-\infty}^{\theta_0}\pi(\theta)\pi(\psi)[f_n(x_1|\theta)f_n(x_2|\psi)-f_n(x_2|\theta)f_n(x_1|\psi)]\;d\psi -->
<!--   d\theta\] -->

<!-- y el denominador es siempre positivo, por lo que basta con que el numerador sea -->
<!-- negativo. -->

<!-- Como $f_n(x|\theta)$ tiene un MLR, si $r(x_1)\leq r(x_2)$ y -->
<!-- $-\infty <\psi\leq \theta_0\leq \theta<+\infty$, -->
<!-- \[\dfrac{f_n(x_1|\theta)}{f_n(x_1|\psi)}\leq\dfrac{f_n(x_2|\theta)}{f_n(x_2|\psi)} \Leftrightarrow f_n(x_1|\theta)f_n(x_2|\psi)\leq f_n(x_2|\theta)f_n(x_1|\psi) \] -->

<!-- Entonces $l(x_1)\leq l(x_2)$ y por tanto ambas pruebas son equivalentes. -->

<!-- **Ejemplo**. Diferencias porcentuales entre calorías observadas y calorías en -->
<!-- publicidad para 20 productos preparados. -->

<!-- $X_1,\dots,X_{20}\sim N(\theta,100)$, $\theta\sim N(0,60)$ -->

<!-- La media posterior es  -->

<!-- \[\dfrac{100\cdot 0 + 20\cdot 60\bar X_{20}}{100+20\cdot 60} = 0.923\bar -->
<!-- X_{20}.\] -->

<!-- y $\sigma_1^2 = 4.62$. -->

<!-- La hipótesis de interés es $H_0:\theta\leq 0$ vs $H_1:\theta>0$. -->

<!-- $\delta$: Rechazo $H_0$ si $\mathbb P[H_0|\bar X_{20}]\leq -->
<!-- \dfrac{w_1}{w_0+w_1}$, donde -->

<!-- \[\mathbb P[\theta\leq 0|\bar X_{20}] = \mathbb P\bigg[Z\bigg|\dfrac{-0.923\bar -->
<!-- X_{20}}{4.62}\bigg] = \Phi(-0.429\bar X_{20}).\] -->

<!-- Bajo $\delta$: -->

<!-- \begin{align*}  -->
<!-- \Phi(-0.429\bar X_{20})\leq \dfrac{w_1}{w_0+w_1}=\beta & \implies -->
<!-- 	-0.429\bar X_{20}\leq \Phi^{-1}(\beta) \\ & \implies \bar X_{20} \geq -->
<!-- 	\dfrac{-\Phi^{-1}(\beta)}{0.429}  -->
<!-- \end{align*} -->

<!-- Si $w_0 = w_1 \implies \beta = 1/2$ y $\Phi(1/2) =0$. Por lo tanto $\bar -->
<!-- X_{20}\geq 0$. -->

<!-- **Interpretación**. Si $\bar X_{20}\geq c$ entonces aceptamos la hipótesis de -->
<!-- que $\theta>0$ (en términos de la aplicación). -->






<!-- ## Hipótesis de 2 colas -->

<!-- \[H_0: \theta = \theta_0 \text{ vs } H_1:\theta \ne \theta_0\] -->

<!-- La significancia práctica indica que "ser igual a $\theta_0$ significa estar -->
<!-- cerca". -->

<!-- Replanteamos $H_0$, tomando $d>0$: -->
<!-- \[H_0: |\theta-\theta_0|\leq d \text{ vs } H_1: |\theta-\theta_0|>d.\] -->

<!-- En el ejemplo anterior, $(\theta_0 = 0)$ -->

<!-- \[\mathbb P[H_0|\bar X_{20}]=\mathbb P[|\theta|\leq d|\bar X_{20}] = -->
<!-- \Phi\left(\dfrac{d-0.1154}{4.62^{1/2}}\right) - -->
<!-- \Phi\left(\dfrac{-d-0.1154}{4.62^{1/2}}\right) = g(d)\] -->

<!-- En el caso normal, si $X_1,\dots,X_n\sim N(\mu,\sigma^2)$ ambos parámetros -->
<!-- desconocidos y $\tau = \dfrac 1{\sigma^2}$, recuerde que -->

<!-- \[[\mu,\tau]\sim \text{Normal-Gamma}(\mu_0,\lambda_0,\alpha_0,\beta_0)\implies -->
<!-- \left(\dfrac{\lambda_0\alpha_0}{\beta_0}\right)^{\frac 12}(\mu-\mu_0)\sim -->
<!-- t_{2\alpha_0}\]  -->

<!-- y la marginal de $\mu$ se usa en el cálculo $\mathbb P[H_0|x]$. -->

<!-- **Ejemplo**. Residuos de un pesticida en apio. $X_1,\dots, X_{77}\sim -->
<!-- N(\mu,\sigma^2)$. Usamos una previa impropia de $(\mu,\sigma^2)$, -->

<!-- \[\pi(\mu,\tau)\propto \tau^{-1}\]. -->

<!-- Recuerde, además, que -->

<!-- \[U = \left(\dfrac{n(n-1)}{s_n^2}\right)^{\frac 12}(\mu-\bar X_n)\sim t_{n-1}\] -->
<!-- en el nivel posterior. -->

<!-- Nos interesa probar $H_0: \mu\geq 55$ vs $H_1:\mu<55$. -->

<!-- Los datos son $\bar X_{77} = 50.23$, $s_{77}^2=34106$. -->

<!-- \begin{align*} -->
<!-- 	\mathbb P[H_0|X] & = \mathbb P[\mu\geq 55|X] \\ & = \mathbb -->
<!--      P\left[\dfrac{\mu-\bar -->
<!--      X_{77}}{\left(\frac{\sigma_2'}{77}\right)^{1/2}}\right] \leq \mathbb -->
<!--      P\left[\dfrac{55-50.23}{\left(\frac{\sigma_2'}{77}\right)^{1/2}}\right] \\ -->
<!--      & = 1-T_{76}[1.974] = 0.026 -->
<!-- \end{align*} -->

<!-- Note que -->
<!-- $\dfrac{-\bar X_{77}-\overbrace{55}^{\mu_0} }{\left(\frac{\sigma_2'}{77}\right)^{1/2}} = -U$ -->

<!-- donde $U$ es el estadístico de prueba en el caso frecuentista y -->

<!-- \[\mathbb P[H_0|X] = \mathbb P[\underbrace{t_{n-1}\geq -\overbrace{u}^{\text{Observado}}}_{\text{Región de rechazo}\\\text{en la prueba frecuentista}}|X]\leq \alpha_0\] -->

<!-- *Interpretación*: aceptamos la hipótesis de que el valor medio del pesticida es -->
<!-- menor o igual a 55 ante una función de pérdida en donde $w_0 = w_1$. -->

<!-- **Teorema**. Sean $X_1,\dots,X_m\sim N(\mu_1,\tau)$ y -->
<!-- $Y_1,\dots, Y_n\sim N(\mu_2,\tau)$ dos muestras y -->
<!-- $\pi(\mu_1,\mu_2,\tau)\propto \tau^{-1}$, $\tau > 0$. Entonces -->
<!-- \[(m+n-2)^{1/2}\dfrac{\mu_1-\mu_2-(\bar X_{m}-\bar Y_n)}{\left(\dfrac 1m + \dfrac 1n\right)^{1/2}(s_X^2+s_Y^2)^{1/2}}\sim t_{m+n-2}\] -->
<!-- condicionado en $(X,Y)$. -->

<!-- *Prueba*: Ejercicio. -->

<!-- **Consecuencia**. Si queremos probar $H_0: \mu_1-\mu_2\leq 0$ vs -->
<!-- $H_1:\mu_1-\mu_2>0$, \[\mathbb P[\mu_1-\mu_2\leq 0|x,y] = \mathbb -->
<!-- P\left[t_{m+n-2}\leq \dfrac{-(\bar X_m-\bar Y_n)}{\left(\dfrac 1m + \dfrac -->
<!-- 1n\right)^{1/2}(s_X^2+s_Y^2)^{1/2}}(m+n-2)^{1/2}\right] = T_{m+n-2})(-u).\] -->

<!-- donde $u$ es el valor observado de la prueba de 2 muestras en el caso -->
<!-- frecuentista. -->

<!-- Rechazamos $H_0$ si -->

<!-- \[ T_{m+n-2}(-u)\leq \dfrac{w_1}{w_0+w_1}=\alpha_0 \Leftrightarrow -u\leq T_{m+n-2}^{-1}(\alpha_0) \implies u\geq -T_{m+n-2}^{-1}(\alpha_0) = T_{m+n-2}^{-1}(1-\alpha_0)\] -->

<!-- Es la misma prueba con $\alpha_0 = \dfrac{w_1}{w_0+w_1}$ con distinta -->
<!-- interpretación. -->

<!-- Otro caso particular es la prueba de varianzas en el caso normal con previas -->
<!-- impropias. -->
